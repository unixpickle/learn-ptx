.version 7.0
.target sm_50 // enough for my Titan X
.address_size 64

// This is similar to matmul_big_blocks_v2.ptx, but it computes 64x64 chunks at
// once, by iterating 64x32 and 32x64 products.

// Load B in a way that is strided like this, so that loading the Nth value
// from every thread in a warp will cause no bank conflicts:
//     01234567 01234567 01234567 01234567 70123456 70123456 70123456 70123456
//     ...    x7   ...
//     67012345 67012345 67012345 67012345 56701234 56701234 56701234 56701234
//     ...    x7   ...
//     45670123 45670123 45670123 45670123 34567012 34567012 34567012 34567012
//     ...    x7   ...
//     23456701 23456701 23456701 23456701 23456701 12345670 12345670 12345670

.visible .entry bigBlocksMatmulV3 (
    .param .u64 ptrA,
    .param .u64 ptrB,
    .param .u64 ptrOut,
    .param .u32 numBlocks
) {
    .reg .pred %p0;
    .reg .u64 %dtmp<2>;
    .reg .u32 %stmp<5>;

    // Attributes of the thread/CTA.
    .reg .u32 %tidX;
    .reg .u32 %tidY;
    .reg .u64 %ctaX;
    .reg .u64 %ctaY;

    // Arguments
    .reg .u64 %ptrA;
    .reg .u64 %ptrB;
    .reg .u64 %ptrOut;
    .reg .u32 %numBlocks;

    // Loaded vector from A and B
    .reg .f32 %colA<8>;
    .reg .f32 %rowB<8>;
    // Accumulated output block to sync across threads at the end.
    .reg .f32 %outBlock<64>;

    // Outer-loop variable.
    .reg .u64 %i;

    // Cache of our column in A, to be broadcast within each warp.
    .reg .f32 %loadedA<8>;

    // Cache of current 32x64 block loaded from global memory.
    // The entries are strided to avoid shared memory bank
    // conflicts.
    .shared .align 4 .f32 loadedB[2048];

    ld.param.u64 %ptrA, [ptrA];
    ld.param.u64 %ptrB, [ptrB];
    ld.param.u64 %ptrOut, [ptrOut];
    ld.param.u32 %numBlocks, [numBlocks];

    mov.u32 %tidX, %tid.x;
    mov.u32 %tidY, %tid.y;
    cvt.u64.u32 %ctaX, %ctaid.x;
    cvt.u64.u32 %ctaY, %ctaid.y;

    // Zero-out accumulation registers (this was auto-generated).
    mov.f32 %outBlock0, 0.0;
    mov.f32 %outBlock1, 0.0;
    mov.f32 %outBlock2, 0.0;
    mov.f32 %outBlock3, 0.0;
    mov.f32 %outBlock4, 0.0;
    mov.f32 %outBlock5, 0.0;
    mov.f32 %outBlock6, 0.0;
    mov.f32 %outBlock7, 0.0;
    mov.f32 %outBlock8, 0.0;
    mov.f32 %outBlock9, 0.0;
    mov.f32 %outBlock10, 0.0;
    mov.f32 %outBlock11, 0.0;
    mov.f32 %outBlock12, 0.0;
    mov.f32 %outBlock13, 0.0;
    mov.f32 %outBlock14, 0.0;
    mov.f32 %outBlock15, 0.0;
    mov.f32 %outBlock16, 0.0;
    mov.f32 %outBlock17, 0.0;
    mov.f32 %outBlock18, 0.0;
    mov.f32 %outBlock19, 0.0;
    mov.f32 %outBlock20, 0.0;
    mov.f32 %outBlock21, 0.0;
    mov.f32 %outBlock22, 0.0;
    mov.f32 %outBlock23, 0.0;
    mov.f32 %outBlock24, 0.0;
    mov.f32 %outBlock25, 0.0;
    mov.f32 %outBlock26, 0.0;
    mov.f32 %outBlock27, 0.0;
    mov.f32 %outBlock28, 0.0;
    mov.f32 %outBlock29, 0.0;
    mov.f32 %outBlock30, 0.0;
    mov.f32 %outBlock31, 0.0;
    mov.f32 %outBlock32, 0.0;
    mov.f32 %outBlock33, 0.0;
    mov.f32 %outBlock34, 0.0;
    mov.f32 %outBlock35, 0.0;
    mov.f32 %outBlock36, 0.0;
    mov.f32 %outBlock37, 0.0;
    mov.f32 %outBlock38, 0.0;
    mov.f32 %outBlock39, 0.0;
    mov.f32 %outBlock40, 0.0;
    mov.f32 %outBlock41, 0.0;
    mov.f32 %outBlock42, 0.0;
    mov.f32 %outBlock43, 0.0;
    mov.f32 %outBlock44, 0.0;
    mov.f32 %outBlock45, 0.0;
    mov.f32 %outBlock46, 0.0;
    mov.f32 %outBlock47, 0.0;
    mov.f32 %outBlock48, 0.0;
    mov.f32 %outBlock49, 0.0;
    mov.f32 %outBlock50, 0.0;
    mov.f32 %outBlock51, 0.0;
    mov.f32 %outBlock52, 0.0;
    mov.f32 %outBlock53, 0.0;
    mov.f32 %outBlock54, 0.0;
    mov.f32 %outBlock55, 0.0;
    mov.f32 %outBlock56, 0.0;
    mov.f32 %outBlock57, 0.0;
    mov.f32 %outBlock58, 0.0;
    mov.f32 %outBlock59, 0.0;
    mov.f32 %outBlock60, 0.0;
    mov.f32 %outBlock61, 0.0;
    mov.f32 %outBlock62, 0.0;
    mov.f32 %outBlock63, 0.0;

    mov.u64 %i, 0;
loop_start:
    // Load the region of A into registers across the block.
    // We will go from j=0 to j=8 of the following:
    //   A[i*32 + tid.x + numBlocks*64*(ctaid.y*64 + tid.y*8 + j)]
    // = A[i*32 + tid.x + numBlocks*64*(ctaid.y*64 + tid.y*8) + j*numBlocks*64]
    shl.b64 %dtmp1, %ctaY, 6; // ctaid.y*64
    cvt.u64.u32 %dtmp0, %tidY;
    shl.b64 %dtmp0, %dtmp0, 4;
    add.u64 %dtmp0, %dtmp0, %dtmp1; // tid.y*8 + ctaid.y*64
    cvt.u64.u32 %dtmp1, %numBlocks;
    mul.lo.u64 %dtmp0, %dtmp0, %dtmp1;
    shl.b64 %dtmp0, %dtmp0, 6; // 64*numBlocks*(ctaid.y*32+tid.y)
    cvt.u64.u32 %dtmp1, %tidX;
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    shl.b64 %dtmp1, %i, 5; // i*32
    add.u64 %dtmp0, %dtmp0, %dtmp1; // i*32 + tid.x + 64*numBlocks*(ctaid.y*32+tid.y)
    shl.b64 %dtmp0, %dtmp0, 2; // byte offset instead of float offset
    mov.u64 %dtmp1, %ptrA;
    add.u64 %dtmp0, %dtmp0, %dtmp1;

    // Stride will be numBlocks*64*4 bytes.
    cvt.u64.u32 %dtmp1, %numBlocks;
    shl.b64 %dtmp1, %dtmp1, 8;

    ld.global.f32 %loadedA0, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA1, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA2, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA3, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA4, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA5, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA6, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA7, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;

    // We don't want to overwrite memory that is currently being read.
    bar.sync 0;

    // Loop over loads from global memory of B
    mov.u32 %stmp0, 0;
load_loop_B_start:
    // Load the region of B into shared memory.
    // We will "iterate" through j=0 and j=1, to copy the left and right
    // halves of the matrix.
    // Offset is B[64*ctaid.x + tid.x + j*32 + 64*numBlocks*(i*32+tid.y+stmp0*8)]
    cvt.u64.u32 %dtmp0, %tidY;
    shl.b64 %dtmp1, %i, 5; // i*32
    add.u64 %dtmp0, %dtmp0, %dtmp1; // i*32 + tid.y
    cvt.u64.u32 %dtmp1, %stmp0;
    shl.b64 %dtmp1, %dtmp1, 3; // stmp0 * 8
    add.u64 %dtmp0, %dtmp0, %dtmp1; // i*32 + tid.y + stmp0*8
    cvt.u64.u32 %dtmp1, %numBlocks;
    mul.lo.u64 %dtmp0, %dtmp0, %dtmp1;
    shl.b64 %dtmp0, %dtmp0, 5; // 32*numBlocks*(i*32+tid.y+stmp0*8)
    shl.b64 %dtmp1, %ctaX, 5; // ctaid.x*32
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    cvt.u64.u32 %dtmp1, %tidX;
    add.u64 %dtmp0, %dtmp0, %dtmp1; // 32*ctaid.x + tid.x + 32*numBlocks*(i*32+tid.y+stmp0*8)
    shl.b64 %dtmp0, %dtmp0, 2; // byte offset rather than float offset
    mov.u64 %dtmp1, %ptrB;
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %rowB0, [%dtmp0];
    // Offset by 32*4 bytes to read second column.
    add.u64 %dtmp1, %dtmp0, 128;
    ld.global.f32 %rowB1, [%dtmp1];

    // Offset in loadedB is loadedB[(tid.x+stmp0*2+j)%32 + j*32 + tid.y*64 + stmp0*8*64]
    shl.b32 %stmp1, %tidY, 8; // tid.y*64*4
    shl.b32 %stmp2, %stmp0, 11; // stmp0*8*64*4
    add.u32 %stmp1, %stmp1, %stmp2; // (tid.y*64 + stmp0*8*64)*4
    mov.u32 %stmp2, loadedB;
    add.u32 %stmp1, %stmp1, %stmp2; // &loadedB[tid.y*64 + stmp0*8*64]

    shl.b32 %stmp2, %stmp0, 3; // stmp0*2*4
    shl.b32 %stmp3, %tidX, 2; // tid.x*4
    add.u32 %stmp2, %stmp3, %stmp2; // (tid.x + stmp0*2)*4
    add.u32 %stmp3, %stmp2, 4; // (tid.x + stmp0*2 + 1)*4
    and.b32 %stmp2, %stmp2, 127; // ((tid.x+stmp0*2) % 32)*4
    and.b32 %stmp3, %stmp3, 127; // ((tid.x+stmp0*2+1) % 32)*4
    add.u32 %stmp3, %stmp3, 128; // ((tid.x+stmp0*2+1) % 32 + 32)*4

    add.u32 %stmp2, %stmp2, %stmp1;
    st.shared.f32 [%stmp2], %rowB0;
    add.u32 %stmp3, %stmp3, %stmp1;
    st.shared.f32 [%stmp3], %rowB1;

    add.u32 %stmp0, %stmp0, 1;
    setp.lt.u32 %p0, %stmp0, 4;
    @%p0 bra load_loop_B_start;
load_loop_B_end:

    // Wait for all copies to be complete.
    bar.sync 0;

    // We will loop from B-row j=0 to j=7, loading segments
    // of eight values k=0 to k=7 from B.
    // loadedB[(k+tid.x//4)%8 + 8*(tid.x%8) + j*64 + 8*64*(tid.x//8)]

    // Inner offset, which we cycle, is ((tid.x//4+k)%8)*4 = (tid.x//4 + k%8)*4.
    // Outer offset is (8*(tid.x%8) + j*64 + 8*64*(tid.x//8))*4
    // which we increment by 64*4=256 each loop, starting at
    //   (8*(tid.x%8) + 8*64*(tid.x//8))*4
    // = ((tid.x&7) << 5) + ((tid.x&0x18) << 8)

    // We can store the outer offset in stmp1, starting at j=0.
    // stmp1 = ((tid.x&7) << 5) + ((tid.x&0x18) << 8)
    and.b32 %stmp1, %tidX, 7;
    shl.b32 %stmp1, %stmp1, 5;
    and.b32 %stmp2, %tidX, 0x18;
    shl.b32 %stmp2, %stmp2, 8;
    add.u32 %stmp1, %stmp1, %stmp2;

    // We can store the inner offset helper as stmp2=(tid.x//4)*4.
    and.b32 %stmp2, %tidX, 0x1C;

    mov.u32 %stmp0, 0;
block_mul_loop_start:
    // We want the registers from warp rank 8*(tid.x//8) + stmp0
    and.b32 %stmp3, %tidX, 0x18;
    add.u32 %stmp3, %stmp3, %stmp0;
    shfl.sync.idx.b32 %colA0, %loadedA0, %stmp3, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA1, %loadedA1, %stmp3, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA2, %loadedA2, %stmp3, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA3, %loadedA3, %stmp3, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA4, %loadedA4, %stmp3, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA5, %loadedA5, %stmp3, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA6, %loadedA6, %stmp3, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA7, %loadedA7, %stmp3, 0x1f, 0xffffffff;

    // To load from B, we must increment stmp2 by 4 and modulo 8*4 eight times.
    // We globally offset by stmp1, which is the j-dependent offset which does
    // not depend on k.
    mov.u32 %stmp3, loadedB;
    add.u32 %stmp3, %stmp3, %stmp1;
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB0, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB1, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB2, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB3, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB4, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB5, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB6, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;
    ld.shared.f32 %rowB7, [%stmp4];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 31; // modulo (4*8)
    add.u32 %stmp4, %stmp3, %stmp2;

    // 8x8 outer product.
    //     for i in range(8):
    //         for j in range(8):
    //             print(f'fma.rn.f32 %outBlock{j+i*8}, %colA{i}, %rowB{j}, %outBlock{j+i*8};')
    fma.rn.f32 %outBlock0, %colA0, %rowB0, %outBlock0;
    fma.rn.f32 %outBlock1, %colA0, %rowB1, %outBlock1;
    fma.rn.f32 %outBlock2, %colA0, %rowB2, %outBlock2;
    fma.rn.f32 %outBlock3, %colA0, %rowB3, %outBlock3;
    fma.rn.f32 %outBlock4, %colA0, %rowB4, %outBlock4;
    fma.rn.f32 %outBlock5, %colA0, %rowB5, %outBlock5;
    fma.rn.f32 %outBlock6, %colA0, %rowB6, %outBlock6;
    fma.rn.f32 %outBlock7, %colA0, %rowB7, %outBlock7;
    fma.rn.f32 %outBlock8, %colA1, %rowB0, %outBlock8;
    fma.rn.f32 %outBlock9, %colA1, %rowB1, %outBlock9;
    fma.rn.f32 %outBlock10, %colA1, %rowB2, %outBlock10;
    fma.rn.f32 %outBlock11, %colA1, %rowB3, %outBlock11;
    fma.rn.f32 %outBlock12, %colA1, %rowB4, %outBlock12;
    fma.rn.f32 %outBlock13, %colA1, %rowB5, %outBlock13;
    fma.rn.f32 %outBlock14, %colA1, %rowB6, %outBlock14;
    fma.rn.f32 %outBlock15, %colA1, %rowB7, %outBlock15;
    fma.rn.f32 %outBlock16, %colA2, %rowB0, %outBlock16;
    fma.rn.f32 %outBlock17, %colA2, %rowB1, %outBlock17;
    fma.rn.f32 %outBlock18, %colA2, %rowB2, %outBlock18;
    fma.rn.f32 %outBlock19, %colA2, %rowB3, %outBlock19;
    fma.rn.f32 %outBlock20, %colA2, %rowB4, %outBlock20;
    fma.rn.f32 %outBlock21, %colA2, %rowB5, %outBlock21;
    fma.rn.f32 %outBlock22, %colA2, %rowB6, %outBlock22;
    fma.rn.f32 %outBlock23, %colA2, %rowB7, %outBlock23;
    fma.rn.f32 %outBlock24, %colA3, %rowB0, %outBlock24;
    fma.rn.f32 %outBlock25, %colA3, %rowB1, %outBlock25;
    fma.rn.f32 %outBlock26, %colA3, %rowB2, %outBlock26;
    fma.rn.f32 %outBlock27, %colA3, %rowB3, %outBlock27;
    fma.rn.f32 %outBlock28, %colA3, %rowB4, %outBlock28;
    fma.rn.f32 %outBlock29, %colA3, %rowB5, %outBlock29;
    fma.rn.f32 %outBlock30, %colA3, %rowB6, %outBlock30;
    fma.rn.f32 %outBlock31, %colA3, %rowB7, %outBlock31;
    fma.rn.f32 %outBlock32, %colA4, %rowB0, %outBlock32;
    fma.rn.f32 %outBlock33, %colA4, %rowB1, %outBlock33;
    fma.rn.f32 %outBlock34, %colA4, %rowB2, %outBlock34;
    fma.rn.f32 %outBlock35, %colA4, %rowB3, %outBlock35;
    fma.rn.f32 %outBlock36, %colA4, %rowB4, %outBlock36;
    fma.rn.f32 %outBlock37, %colA4, %rowB5, %outBlock37;
    fma.rn.f32 %outBlock38, %colA4, %rowB6, %outBlock38;
    fma.rn.f32 %outBlock39, %colA4, %rowB7, %outBlock39;
    fma.rn.f32 %outBlock40, %colA5, %rowB0, %outBlock40;
    fma.rn.f32 %outBlock41, %colA5, %rowB1, %outBlock41;
    fma.rn.f32 %outBlock42, %colA5, %rowB2, %outBlock42;
    fma.rn.f32 %outBlock43, %colA5, %rowB3, %outBlock43;
    fma.rn.f32 %outBlock44, %colA5, %rowB4, %outBlock44;
    fma.rn.f32 %outBlock45, %colA5, %rowB5, %outBlock45;
    fma.rn.f32 %outBlock46, %colA5, %rowB6, %outBlock46;
    fma.rn.f32 %outBlock47, %colA5, %rowB7, %outBlock47;
    fma.rn.f32 %outBlock48, %colA6, %rowB0, %outBlock48;
    fma.rn.f32 %outBlock49, %colA6, %rowB1, %outBlock49;
    fma.rn.f32 %outBlock50, %colA6, %rowB2, %outBlock50;
    fma.rn.f32 %outBlock51, %colA6, %rowB3, %outBlock51;
    fma.rn.f32 %outBlock52, %colA6, %rowB4, %outBlock52;
    fma.rn.f32 %outBlock53, %colA6, %rowB5, %outBlock53;
    fma.rn.f32 %outBlock54, %colA6, %rowB6, %outBlock54;
    fma.rn.f32 %outBlock55, %colA6, %rowB7, %outBlock55;
    fma.rn.f32 %outBlock56, %colA7, %rowB0, %outBlock56;
    fma.rn.f32 %outBlock57, %colA7, %rowB1, %outBlock57;
    fma.rn.f32 %outBlock58, %colA7, %rowB2, %outBlock58;
    fma.rn.f32 %outBlock59, %colA7, %rowB3, %outBlock59;
    fma.rn.f32 %outBlock60, %colA7, %rowB4, %outBlock60;
    fma.rn.f32 %outBlock61, %colA7, %rowB5, %outBlock61;
    fma.rn.f32 %outBlock62, %colA7, %rowB6, %outBlock62;
    fma.rn.f32 %outBlock63, %colA7, %rowB7, %outBlock63;

    // Offset in B gets incremented by 64*4 bytes.
    add.u32 %stmp1, %stmp1, 256;

    add.u32 %stmp0, %stmp0, 1;
    setp.lt.u32 %p0, %stmp0, 8;
    @%p0 bra block_mul_loop_start;
block_mul_loop_end:

    add.u64 %i, %i, 1;
    cvt.u64.u32 %dtmp0, %numBlocks;
    setp.lt.u64 %p0, %i, %dtmp0;
    @%p0 bra loop_start;
loop_end:

    // We will reduce into loadedB, so we initially zero it out.
    bar.sync 0;
    shl.b32 %stmp0, %tidY, 5; // tid.y * 32
    add.u32 %stmp0, %stmp0, %tidX;
    shl.b32 %stmp0, %stmp0, 2; // (tid.y*32 + tid.x) * 4
    mov.u32 %stmp1, loadedB;
    add.u32 %stmp1, %stmp1, %stmp0;
    st.shared.f32 [%stmp1], 0.0;
    st.shared.f32 [%stmp1+1024], 0.0;
    st.shared.f32 [%stmp1+2048], 0.0;
    st.shared.f32 [%stmp1+3072], 0.0;
    st.shared.f32 [%stmp1+4096], 0.0;
    st.shared.f32 [%stmp1+5120], 0.0;
    st.shared.f32 [%stmp1+6144], 0.0;
    st.shared.f32 [%stmp1+7168], 0.0;
    bar.sync 0;

    // TODO: accumulate the output blocks into loadedB.
    // We will have to do this in two stages, since loadedB
    // is only half the size of our output.

    ret;
}
