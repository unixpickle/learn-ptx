.version 7.0
.target sm_50 // enough for my Titan X
.address_size 64

.visible .entry simpleMatmul (
    .param .u64 ptrA,
    .param .u64 ptrB,
    .param .u64 ptrOut,
    .param .u32 numBlocks
) {
    .reg .pred %p0;
    .reg .u64  %dtmp<2>;
    .reg .u32  %stmp<2>;

    // Attributes of the thread/CTA.
    .reg .u32  %blockSize;
    .reg .u32  %tidX;
    .reg .u32  %tidY;

    .reg .u64  %offsetX;
    .reg .u64  %offsetY;
    .reg .u64  %stride;
    .reg .u32  %i;
    .reg .u32  %numIters;
    .reg .u32  %numBlocks;
    .reg .u64  %ptrA;
    .reg .u64  %ptrB;
    .reg .u64  %ptrOut;
    .reg .f32  %acc;
    .reg .f32  %val<2>;

    ld.param.u32 %numBlocks, [numBlocks];
    ld.param.u64 %ptrA, [ptrA];
    ld.param.u64 %ptrB, [ptrB];
    ld.param.u64 %ptrOut, [ptrOut];

    mov.u32 %blockSize, %ntid.x;
    mov.u32 %tidX, %tid.x;
    mov.u32 %tidY, %tid.y;

    // For computing offsetX, offsetY, and stride, we use
    // %dtmp0 to store a 64-bit version of %blockSize.
    cvt.u64.u32 %dtmp0, %blockSize; // %dtmp0 = %blockSize

    // Compute offsets in the output matrix.
    // offsetX = ctaid.x * ntid.x = ctaid.x * blockSize
    cvt.u64.u32 %offsetX, %ctaid.x;
    mul.lo.u64 %offsetX, %offsetX, %dtmp0;
    // offsetY = ctaid.y * ntid.y = ctaid.y * blockSize
    cvt.u64.u32 %offsetY, %ctaid.y;
    mul.lo.u64 %offsetY, %offsetY, %dtmp0;

    // Stride is blockSize * numBlocks;
    cvt.u64.u32 %stride, %numBlocks;
    mul.lo.u64 %stride, %stride, %dtmp0;

    // We will accumulate into this register.
    mov.f32 %acc, 0.0;

    // We will always read from A in order starting at
    // stride*(offsetY+tid.y) and going forward one element
    // per inner-loop iteration.
    cvt.u64.u32 %dtmp0, %tidY;
    add.u64 %dtmp0, %dtmp0, %offsetY;
    mul.lo.u64 %dtmp0, %dtmp0, %stride;
    shl.b64 %dtmp0, %dtmp0, 2;
    add.u64 %ptrA, %ptrA, %dtmp0;

    // We will calculate our block offset in B in %ptrB as
    // (offsetX + tid.x, i*ntid.y+j)
    // = offsetX + tid.x + (i*blockSize+j)*stride
    cvt.u64.u32 %dtmp1, %tidX;
    add.u64 %dtmp0, %dtmp1, %offsetX;
    shl.b64 %dtmp0, %dtmp0, 2;
    add.u64 %ptrB, %ptrB, %dtmp0;

    // Set %dtmp1 to stride in B.
    // Stride in ptrB is stride*4
    shl.b64 %dtmp1, %stride, 2;

    mul.lo.u32 %numIters, %blockSize, %numBlocks;
    mov.u32 %i, 0;
loop_start:
    // We are iterating through the entire row of A sequentially.
    ld.global.f32 %val0, [%ptrA];
    add.u64 %ptrA, %ptrA, 4;

    // Iterate through the entire column of B sequentially.
    ld.global.f32 %val1, [%ptrB];
    add.u64 %ptrB, %ptrB, %dtmp1;

    // This will be optimized to a fused operation.
    mul.f32 %val1, %val0, %val1;
    add.f32 %acc, %acc, %val1;

    // i += 1; loop while i < numBlocks
    add.u32 %i, %i, 1;
    setp.lt.u32 %p0, %i, %numIters;
    @%p0 bra loop_start;

loop_end:
    // Write back to output memory.

    // Output address is offsetX+tid.x + stride*(offsetY+tid.y)
    cvt.u64.u32 %dtmp0, %tidY;
    add.u64 %dtmp0, %dtmp0, %offsetY;
    mul.lo.u64 %dtmp0, %dtmp0, %stride;
    cvt.u64.u32 %dtmp1, %tidX;
    add.u64 %dtmp1, %dtmp1, %offsetX;
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    shl.b64 %dtmp0, %dtmp0, 2;
    add.u64 %dtmp0, %dtmp0, %ptrOut;

    st.global.f32 [%dtmp0], %acc;
}
