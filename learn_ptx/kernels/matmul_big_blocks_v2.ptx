.version 7.0
.target sm_50 // enough for my Titan X
.address_size 64

// This is similar to matmul_big_blocks.ptx, except that the
// portion of A is loaded into registers instead of shared
// memory to increase occupancy.

.visible .entry bigBlocksMatmulV2 (
    .param .u64 ptrA,
    .param .u64 ptrB,
    .param .u64 ptrOut,
    .param .u32 numBlocks
) {
    .reg .pred %p0;
    .reg .u64 %dtmp<2>;
    .reg .u32 %stmp<6>;

    // Attributes of the thread/CTA.
    .reg .u32 %tidX;
    .reg .u32 %tidY;
    .reg .u64 %ctaX;
    .reg .u64 %ctaY;

    // Arguments
    .reg .u64 %ptrA;
    .reg .u64 %ptrB;
    .reg .u64 %ptrOut;
    .reg .u32 %numBlocks;

    // Loaded vector from A and B
    .reg .f32 %colA<4>;
    .reg .f32 %rowB<4>;
    // Accumulated output block which we need to add across four threads in
    // each warp at the end.
    .reg .f32 %outBlock<16>;

    // Outer-loop variable.
    .reg .u64 %i;

    // Cache of our column in A, to be broadcast within each warp.
    .reg .f32 %loadedA<4>;

    // Cache of current 32x32 block loaded from global memory.
    .shared .align 4 .f32 loadedB[1024];

    ld.param.u64 %ptrA, [ptrA];
    ld.param.u64 %ptrB, [ptrB];
    ld.param.u64 %ptrOut, [ptrOut];
    ld.param.u32 %numBlocks, [numBlocks];

    mov.u32 %tidX, %tid.x;
    mov.u32 %tidY, %tid.y;
    cvt.u64.u32 %ctaX, %ctaid.x;
    cvt.u64.u32 %ctaY, %ctaid.y;

    mov.f32 %outBlock0, 0.0;
    mov.f32 %outBlock1, 0.0;
    mov.f32 %outBlock2, 0.0;
    mov.f32 %outBlock3, 0.0;
    mov.f32 %outBlock4, 0.0;
    mov.f32 %outBlock5, 0.0;
    mov.f32 %outBlock6, 0.0;
    mov.f32 %outBlock7, 0.0;
    mov.f32 %outBlock8, 0.0;
    mov.f32 %outBlock9, 0.0;
    mov.f32 %outBlock10, 0.0;
    mov.f32 %outBlock11, 0.0;
    mov.f32 %outBlock12, 0.0;
    mov.f32 %outBlock13, 0.0;
    mov.f32 %outBlock14, 0.0;
    mov.f32 %outBlock15, 0.0;

    mov.u64 %i, 0;
loop_start:
    // Load the region of A into registers across the block.
    // We will go from j=0 to j=3 of the following:
    //   A[i*32 + tid.x + numBlocks*32*(ctaid.y*32 + tid.y*4 + j)]
    // = A[i*32 + tid.x + numBlocks*32*(ctaid.y*32 + tid.y*4) + j*numBlocks*32]
    shl.b64 %dtmp1, %ctaY, 5; // ctaid.y*32
    cvt.u64.u32 %dtmp0, %tidY;
    shl.b64 %dtmp0, %dtmp0, 2;
    add.u64 %dtmp0, %dtmp0, %dtmp1; // tid.y*4 + ctaid.y*32
    cvt.u64.u32 %dtmp1, %numBlocks;
    mul.lo.u64 %dtmp0, %dtmp0, %dtmp1;
    shl.b64 %dtmp0, %dtmp0, 5; // 32*numBlocks*(ctaid.y*32+tid.y)
    cvt.u64.u32 %dtmp1, %tidX;
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    shl.b64 %dtmp1, %i, 5; // i*32
    add.u64 %dtmp0, %dtmp0, %dtmp1; // i*32 + tid.x + 1024*numBlocks*(ctaid.y*32+tid.y+stmp0*8)
    shl.b64 %dtmp0, %dtmp0, 2; // byte offset instead of float offset
    mov.u64 %dtmp1, %ptrA;
    add.u64 %dtmp0, %dtmp0, %dtmp1;

    // Stride will be numBlocks*32*4 bytes.
    cvt.u64.u32 %dtmp1, %numBlocks;
    shl.b64 %dtmp1, %dtmp1, 7;

    ld.global.f32 %loadedA0, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA1, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA2, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %loadedA3, [%dtmp0];
    add.u64 %dtmp0, %dtmp0, %dtmp1;

    // We don't want to overwrite memory that is currently being read.
    bar.sync 0;

    // Loop over loads from global memory of B
    mov.u32 %stmp0, 0;
load_loop_B_start:
    // Load the region of B into shared memory.
    // Offset is B[32*ctaid.x + tid.x + 32*numBlocks*(i*32+tid.y+stmp0*8)]
    cvt.u64.u32 %dtmp0, %tidY;
    shl.b64 %dtmp1, %i, 5; // i*32
    add.u64 %dtmp0, %dtmp0, %dtmp1; // i*32 + tid.y
    cvt.u64.u32 %dtmp1, %stmp0;
    shl.b64 %dtmp1, %dtmp1, 3; // stmp0 * 8
    add.u64 %dtmp0, %dtmp0, %dtmp1; // i*32 + tid.y + stmp0*8
    cvt.u64.u32 %dtmp1, %numBlocks;
    mul.lo.u64 %dtmp0, %dtmp0, %dtmp1;
    shl.b64 %dtmp0, %dtmp0, 5; // 32*numBlocks*(i*32+tid.y+stmp0*8)
    shl.b64 %dtmp1, %ctaX, 5; // ctaid.x*32
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    cvt.u64.u32 %dtmp1, %tidX;
    add.u64 %dtmp0, %dtmp0, %dtmp1; // 32*ctaid.x + tid.x + 32*numBlocks*(i*32+tid.y+stmp0*8)
    shl.b64 %dtmp0, %dtmp0, 2; // byte offset rather than float offset
    mov.u64 %dtmp1, %ptrB;
    add.u64 %dtmp0, %dtmp0, %dtmp1;
    ld.global.f32 %rowB0, [%dtmp0];

    // Offset in loadedB is loadedB[(tid.x+stmp0)%32 + tid.y*32 + stmp0*8*32]
    add.u32 %stmp2, %tidX, %stmp0;
    and.b32 %stmp2, %stmp2, 0x1f; // (tid.x+stmp0) % 32
    shl.b32 %stmp1, %tidY, 5; // tid.y*32
    add.u32 %stmp1, %stmp1, %stmp2; // (tid.x+stmp0)%32 + tid.y*32
    shl.b32 %stmp2, %stmp0, 8; // stmp0*8*32
    add.u32 %stmp1, %stmp1, %stmp2;
    shl.b32 %stmp1, %stmp1, 2; // byte offset instead of float index
    mov.u32 %stmp2, loadedB;
    add.u32 %stmp2, %stmp1, %stmp2;
    st.shared.f32 [%stmp2], %rowB0;

    add.u32 %stmp0, %stmp0, 1;
    setp.lt.u32 %p0, %stmp0, 4;
    @%p0 bra load_loop_B_start;
load_loop_B_end:

    // Wait for all copies to be complete.
    bar.sync 0;

    // We will loop from j=0 to j=7, loading segments of four
    // values k=0 to k=3 from B
    // Load offsets from B as loadedB[(tid.x//8 + 4*(tid.x%8) + k) % 32 + j*32 + 8*32*(tid.x//8)]

    shr.b32 %stmp0, %tidX, 3; // tid.x//8

    // Outer offset for localB.
    // Store stmp3 = 4*(j*32 + 8*32*(tid.x//8))      -- updated per inner loop
    shl.b32 %stmp3, %stmp0, 10; // 4*8*32*(tid.x//8)

    // Inner offset for localB.
    // Store stmp2 = 4*((tid.x//8 + 4*(tid.x%8) + k)%32) -- reverted after every inner loop.
    and.b32 %stmp1, %tidX, 7; // tid.x%8
    shl.b32 %stmp1, %stmp1, 2; // 4*(tid.x%8)
    add.u32 %stmp2, %stmp0, %stmp1; // tid.x//8 + 4*(tid.x%8)
    shl.b32 %stmp2, %stmp2, 2; // 4*(tid.x//8 + 4*(tid.x%8))
    and.b32 %stmp2, %stmp2, 0x7f; // modulo 128

    mov.u32 %stmp0, 0;
block_mul_loop_start:
    // We want the registers from warp rank 8*(tid.x//8) + stmp0
    and.b32 %stmp4, %tidX, 0x18;
    add.u32 %stmp4, %stmp4, %stmp0;
    shfl.sync.idx.b32 %colA0, %loadedA0, %stmp4, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA1, %loadedA1, %stmp4, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA2, %loadedA2, %stmp4, 0x1f, 0xffffffff;
    shfl.sync.idx.b32 %colA3, %loadedA3, %stmp4, 0x1f, 0xffffffff;

    // To load from B, we must increment by 4 and modulo 32*4 four times,
    // once per k, and then revert back by adding 32*4-4*3 = 116 and doing
    // one more modulus.
    // We globally offset by stmp3, which is the j-dependent offset which does
    // not depend on k.
    mov.u32 %stmp4, loadedB;
    add.u32 %stmp4, %stmp4, %stmp3;
    add.u32 %stmp5, %stmp4, %stmp2;
    ld.shared.f32 %rowB0, [%stmp5];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 0x7f;
    add.u32 %stmp5, %stmp4, %stmp2;
    ld.shared.f32 %rowB1, [%stmp5];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 0x7f;
    add.u32 %stmp5, %stmp4, %stmp2;
    ld.shared.f32 %rowB2, [%stmp5];
    add.u32 %stmp2, %stmp2, 4;
    and.b32 %stmp2, %stmp2, 0x7f;
    add.u32 %stmp5, %stmp4, %stmp2;
    ld.shared.f32 %rowB3, [%stmp5];
    add.u32 %stmp2, %stmp2, 116;
    and.b32 %stmp2, %stmp2, 0x7f;

    // 4x4 outer product.
    fma.rn.f32 %outBlock0, %colA0, %rowB0, %outBlock0;
    fma.rn.f32 %outBlock1, %colA0, %rowB1, %outBlock1;
    fma.rn.f32 %outBlock2, %colA0, %rowB2, %outBlock2;
    fma.rn.f32 %outBlock3, %colA0, %rowB3, %outBlock3;
    fma.rn.f32 %outBlock4, %colA1, %rowB0, %outBlock4;
    fma.rn.f32 %outBlock5, %colA1, %rowB1, %outBlock5;
    fma.rn.f32 %outBlock6, %colA1, %rowB2, %outBlock6;
    fma.rn.f32 %outBlock7, %colA1, %rowB3, %outBlock7;
    fma.rn.f32 %outBlock8, %colA2, %rowB0, %outBlock8;
    fma.rn.f32 %outBlock9, %colA2, %rowB1, %outBlock9;
    fma.rn.f32 %outBlock10, %colA2, %rowB2, %outBlock10;
    fma.rn.f32 %outBlock11, %colA2, %rowB3, %outBlock11;
    fma.rn.f32 %outBlock12, %colA3, %rowB0, %outBlock12;
    fma.rn.f32 %outBlock13, %colA3, %rowB1, %outBlock13;
    fma.rn.f32 %outBlock14, %colA3, %rowB2, %outBlock14;
    fma.rn.f32 %outBlock15, %colA3, %rowB3, %outBlock15;

    // Offset in B gets incremented by 32*4 bytes.
    add.u32 %stmp3, %stmp3, 128;

    add.u32 %stmp0, %stmp0, 1;
    setp.lt.u32 %p0, %stmp0, 8;
    @%p0 bra block_mul_loop_start;
block_mul_loop_end:

    add.u64 %i, %i, 1;
    cvt.u64.u32 %dtmp0, %numBlocks;
    setp.lt.u64 %p0, %i, %dtmp0;
    @%p0 bra loop_start;
loop_end:

    // We will reduce into loadedB, so we initially zero it out.
    bar.sync 0;
    shl.b32 %stmp0, %tidY, 5; // tid.y * 32
    add.u32 %stmp0, %stmp0, %tidX;
    shl.b32 %stmp0, %stmp0, 2; // (tid.y*32 + tid.x) * 4
    mov.u32 %stmp1, loadedB;
    add.u32 %stmp1, %stmp1, %stmp0;
    st.shared.f32 [%stmp1], 0.0;
    st.shared.f32 [%stmp1+1024], 0.0;
    st.shared.f32 [%stmp1+2048], 0.0;
    st.shared.f32 [%stmp1+3072], 0.0;

    bar.sync 0;

    // Our output block corresponds to the block at row tid.y and tid.x%8.
    // We need to reduce across all tid.x % 8.
    // In particular, we will reduce into loadedB[(tid.y*4+i)*32 + 4*(tid.x%8) + j]
    shl.b32 %stmp0, %tidY, 7; // tid.y*4*32
    and.b32 %stmp1, %tidX, 7; // tid.x%8
    shl.b32 %stmp1, %stmp1, 2; // 4*(tid.x%8)
    add.u32 %stmp0, %stmp0, %stmp1; // tid.y*4*32 + 4*(tid.x%8)
    shl.b32 %stmp0, %stmp0, 2; // convert to byte offset
    mov.u32 %stmp1, loadedB;
    add.u32 %stmp0, %stmp1, %stmp0;

    // Auto-generated code:
    //
    //     for i in range(4):
    //         for j in range(4):
    //             print(f"atom.shared.add.f32 %colA0, [%stmp0+{j*4 + i*32*4}], %outBlock{i*4 + j};")
    //
    atom.shared.add.f32 %colA0, [%stmp0+0], %outBlock0;
    atom.shared.add.f32 %colA0, [%stmp0+4], %outBlock1;
    atom.shared.add.f32 %colA0, [%stmp0+8], %outBlock2;
    atom.shared.add.f32 %colA0, [%stmp0+12], %outBlock3;
    atom.shared.add.f32 %colA0, [%stmp0+128], %outBlock4;
    atom.shared.add.f32 %colA0, [%stmp0+132], %outBlock5;
    atom.shared.add.f32 %colA0, [%stmp0+136], %outBlock6;
    atom.shared.add.f32 %colA0, [%stmp0+140], %outBlock7;
    atom.shared.add.f32 %colA0, [%stmp0+256], %outBlock8;
    atom.shared.add.f32 %colA0, [%stmp0+260], %outBlock9;
    atom.shared.add.f32 %colA0, [%stmp0+264], %outBlock10;
    atom.shared.add.f32 %colA0, [%stmp0+268], %outBlock11;
    atom.shared.add.f32 %colA0, [%stmp0+384], %outBlock12;
    atom.shared.add.f32 %colA0, [%stmp0+388], %outBlock13;
    atom.shared.add.f32 %colA0, [%stmp0+392], %outBlock14;
    atom.shared.add.f32 %colA0, [%stmp0+396], %outBlock15;

    bar.sync 0;

    // We want to dump loadedB into the output.
    // dtmp0 = stride = numBlocks*32
    cvt.u64.u32 %dtmp1, %numBlocks;
    shl.b64 %dtmp0, %dtmp1, 5;

    // We start at ((ctaY*32 + tidY)*stride + ctaX*32 + tidX)*4
    // We trash the %ctaY register at this point, since we don't
    // use it again afterwards.
    shl.b64 %ctaY, %ctaY, 5;
    cvt.u64.u32 %dtmp1, %tidY;
    add.u64 %ctaY, %ctaY, %dtmp1;
    mul.lo.u64 %ctaY, %ctaY, %dtmp0;
    shl.b64 %ctaX, %ctaX, 5;
    add.u64 %ctaY, %ctaY, %ctaX;
    cvt.u64.u32 %dtmp1, %tidX;
    add.u64 %ctaY, %ctaY, %dtmp1;
    shl.b64 %ctaY, %ctaY, 2;
    add.u64 %ptrOut, %ptrOut, %ctaY;

    // The full stride during the copy is stride*8*4.
    shl.b64 %dtmp0, %dtmp0, 5;

    shl.b32 %stmp0, %tidY, 7;
    shl.b32 %stmp1, %tidX, 2;
    add.u32 %stmp0, %stmp0, %stmp1;
    mov.u32 %stmp1, loadedB;
    add.u32 %stmp0, %stmp0, %stmp1;

    ld.shared.f32 %outBlock0, [%stmp0];
    st.global.f32 [%ptrOut], %outBlock0;
    add.u64 %ptrOut, %ptrOut, %dtmp0;

    ld.shared.f32 %outBlock1, [%stmp0+1024]; // 32*8*4
    st.global.f32 [%ptrOut], %outBlock1;
    add.u64 %ptrOut, %ptrOut, %dtmp0;

    ld.shared.f32 %outBlock2, [%stmp0+2048]; // 2*32*8*4
    st.global.f32 [%ptrOut], %outBlock2;
    add.u64 %ptrOut, %ptrOut, %dtmp0;

    ld.shared.f32 %outBlock3, [%stmp0+3072]; // 3*32*8*4
    st.global.f32 [%ptrOut], %outBlock3;

    ret;
}
