.version 7.0
.target sm_80 // needed for wmma instruction
.address_size 64

// Like matmul_wmma_v8.ptx, but uses 4 warps instead of 16 and pre-fetches the
// relevant row from A into registers to use on all 4 columns.
// Likewise, it accumulates an entire row of results instead of just a single 16x16
// block.
// This reduces the reliance on shared memory reads for A, which seem to be a source
// of overhead when using really fast TensorCores.

.visible .entry wmmaMatmulV9 (
    .param .u64 ptrA,
    .param .u64 ptrB,
    .param .u64 ptrOut,
    .param .u32 numBlocks
) {
    .reg .pred %p0;

    // Attributes of the thread/CTA.
    .reg .u32 %tidX;
    .reg .u32 %tidY;
    .reg .u64 %ctaX;
    .reg .u64 %ctaY;

    // Arguments
    .reg .u64 %ptrA;
    .reg .u64 %ptrB;
    .reg .u64 %ptrOut;
    .reg .u32 %numBlocks;

    // Cache for block operands.
    .shared .align 4 .f32 sharedA[4608]; // padding of 8, (64+8)*64
    .shared .align 4 .f32 sharedB[5120]; // padding of 16, (64+16)*64

    ld.param.u64 %ptrA, [ptrA];
    ld.param.u64 %ptrB, [ptrB];
    ld.param.u64 %ptrOut, [ptrOut];
    ld.param.u32 %numBlocks, [numBlocks];

    mov.u32 %tidX, %tid.x; // index in warp (0-32)
    mov.u32 %tidY, %tid.y; // warp index in block (0-16)
    cvt.u64.u32 %ctaX, %ctaid.x; // column of output
    cvt.u64.u32 %ctaY, %ctaid.y; // row of output

    // Accumulation registers are stored as 8 floats per thread.
    .reg .f32 %out<32>;
    // for i in range(32): print('    mov.f32 %out' + str(i) + ', 0.0;')
    mov.f32 %out0, 0.0;
    mov.f32 %out1, 0.0;
    mov.f32 %out2, 0.0;
    mov.f32 %out3, 0.0;
    mov.f32 %out4, 0.0;
    mov.f32 %out5, 0.0;
    mov.f32 %out6, 0.0;
    mov.f32 %out7, 0.0;
    mov.f32 %out8, 0.0;
    mov.f32 %out9, 0.0;
    mov.f32 %out10, 0.0;
    mov.f32 %out11, 0.0;
    mov.f32 %out12, 0.0;
    mov.f32 %out13, 0.0;
    mov.f32 %out14, 0.0;
    mov.f32 %out15, 0.0;
    mov.f32 %out16, 0.0;
    mov.f32 %out17, 0.0;
    mov.f32 %out18, 0.0;
    mov.f32 %out19, 0.0;
    mov.f32 %out20, 0.0;
    mov.f32 %out21, 0.0;
    mov.f32 %out22, 0.0;
    mov.f32 %out23, 0.0;
    mov.f32 %out24, 0.0;
    mov.f32 %out25, 0.0;
    mov.f32 %out26, 0.0;
    mov.f32 %out27, 0.0;
    mov.f32 %out28, 0.0;
    mov.f32 %out29, 0.0;
    mov.f32 %out30, 0.0;
    mov.f32 %out31, 0.0;

    // The row-wise stride of the matrices, measured in tf32's.
    .reg .u64 %stride;
    {
        .reg .u64 %tmp;
        cvt.u64.u32 %tmp, %numBlocks;
        shl.b64 %stride, %tmp, 6;
    }

    // Stride of 1 row.
    .reg .u64 %bytesPerRow;
    shl.b64 %bytesPerRow, %stride, 2; // 4 bytes/float

    // Stride of 64 rows.
    .reg .u64 %loadBStride;
    shl.b64 %loadBStride, %stride, 8; // 64 rows * (4 bytes/float)

    // We want to load 64 rows with 4 warps.
    // Each warp will load 16 consecutive rows, one 32-float column
    // at a time.
    // The pointerInA will point to the thread-specific location in the first
    // row and first column to load.
    // We will store the data in sharedA, and the first location is pointed to
    // by storePointerInSharedA.
    // We will load chunks at a time from sharedA, and the warp-specific pointer
    // to the first block (as determined by the row specified by tidY, is pointed
    // to by loadPointerInSharedA.
    .reg .u64 %pointerInA;
    .reg .u32 %storePointerInSharedA;
    .reg .u32 %loadPointerInSharedA;
    {
        .reg .u64 %tmp<2>;
        .reg .u32 %stmp0;

        cvt.u64.u32 %tmp0, %tidY;
        shl.b64 %tmp0, %tmp0, 4; // tidY * 16 rows
        shl.b64 %tmp1, %ctaY, 6; // ctaY * 64 rows
        add.u64 %tmp0, %tmp0, %tmp1;
        mul.lo.u64 %tmp1, %tmp0, %stride; // floats / row
        cvt.u64.u32 %tmp0, %tidX;
        add.u64 %tmp1, %tmp1, %tmp0;
        shl.b64 %tmp1, %tmp1, 2; // 4 bytes / float
        add.u64 %pointerInA, %tmp1, %ptrA; // &ptrA[4*stride*(tidY*4 + ctaY*64 + tidX)]

        // We load from a row of A, given by tidY.
        mul.lo.u32 %stmp0, %tidY, 4608; // *= (16 rows) * (64+8 floats/row) * (4 bytes/float)
        mov.u32 %loadPointerInSharedA, sharedA;
        add.u32 %loadPointerInSharedA, %loadPointerInSharedA, %stmp0;

        // We will store our row into sharedA similar to how we loaded it.
        // The offset will be ((tidY*16 rows) * (64 + 8 floats/row) + tidX) * (4 bytes/float)
        shl.b32 %stmp0, %tidY, 4; // tidY * 16
        mul.lo.u32 %stmp0, %stmp0, 72; // (64+8)
        add.u32 %stmp0, %stmp0, %tidX;
        shl.b32 %stmp0, %stmp0, 2; // *= 4 bytes/float
        mov.u32 %storePointerInSharedA, sharedA;
        add.u32 %storePointerInSharedA, %storePointerInSharedA, %stmp0;
    }

    // We load B the same way we load A, except that we use padding of 16 instead of 8
    // in shared memory to avoid bank conflicts.
    .reg .u64 %pointerInB;
    .reg .u32 %storePointerInSharedB;
    .reg .u32 %loadPointerInSharedB;
    {
        .reg .u32 %stmp<2>;
        .reg .u64 %tmp<2>;

        shl.b64 %tmp0, %ctaX, 8; // 4 bytes per float * 64 columns
        add.u64 %pointerInB, %ptrB, %tmp0;
        // Offset by tidY*16 rows and tidX columns.
        cvt.u64.u32 %tmp0, %tidY;
        shl.b64 %tmp0, %tmp0, 4; // tidY * 16 rows
        mul.lo.u64 %tmp1, %tmp0, %stride; // floats / row
        cvt.u64.u32 %tmp0, %tidX;
        add.u64 %tmp1, %tmp1, %tmp0;
        shl.b64 %tmp1, %tmp1, 2; // 4 bytes / float
        add.u64 %pointerInB, %pointerInB, %tmp1; // &ptrB[ctaX*64 + stride*4*(tidY*4) + tidX*4]

        // We will store our row into sharedB similar to how we loaded it.
        // The offset will be ((tidY*16 rows) * (64 + 16 floats/row) + tidX) * (4 bytes/float)
        shl.b32 %stmp0, %tidY, 4; // tidY * 16
        mul.lo.u32 %stmp0, %stmp0, 80; // (64+16)
        add.u32 %stmp0, %stmp0, %tidX;
        shl.b32 %stmp0, %stmp0, 2; // *= 4 bytes/float
        mov.u32 %storePointerInSharedB, sharedB;
        add.u32 %storePointerInSharedB, %storePointerInSharedB, %stmp0;

        mov.u32 %loadPointerInSharedB, sharedB;
    }

    .reg .u32 %remainingIters;
    mov.u32 %remainingIters, %numBlocks;

outer_loop:
    setp.gt.u32 %p0, %remainingIters, 0;
    @!%p0 bra outer_loop_end;
    sub.u32 %remainingIters, %remainingIters, 1;

    // Load matrix A into shared memory.
    {
        .reg .f32 %ftmp<8>;
        .reg .b64 %tmp<4>;

        mov.u64 %tmp0, %pointerInA;

        .reg .u32 %storePtr;
        mov.u32 %storePtr, %storePointerInSharedA;

        // Load all 16 rows, four rows at a time.
        .reg .u32 %i;
        mov.u32 %i, 0;
        {
        load_a_start:
            add.u64 %tmp1, %tmp0, %bytesPerRow;
            add.u64 %tmp2, %tmp1, %bytesPerRow;
            add.u64 %tmp3, %tmp2, %bytesPerRow;

            // Load four rows, two columns at a time.
            ld.global.f32 %ftmp0, [%tmp0];
            ld.global.f32 %ftmp1, [%tmp0+128];
            ld.global.f32 %ftmp2, [%tmp1];
            ld.global.f32 %ftmp3, [%tmp1+128];
            ld.global.f32 %ftmp4, [%tmp2];
            ld.global.f32 %ftmp5, [%tmp2+128];
            ld.global.f32 %ftmp6, [%tmp3];
            ld.global.f32 %ftmp7, [%tmp3+128];
    
            // Every other value, we offset (32+8)*4 rather than 32*4.
            st.shared.b32 [%storePtr], %ftmp0;
            st.shared.b32 [%storePtr+128], %ftmp1;
            st.shared.b32 [%storePtr+288], %ftmp2;
            st.shared.b32 [%storePtr+288+128], %ftmp3;
            st.shared.b32 [%storePtr+288*2], %ftmp4;
            st.shared.b32 [%storePtr+288*2+128], %ftmp5;
            st.shared.b32 [%storePtr+288*3], %ftmp6;
            st.shared.b32 [%storePtr+288*3+128], %ftmp7;

            add.u32 %storePtr, %storePtr, 1152; // add 288*4
            add.u64 %tmp0, %tmp3, %bytesPerRow;

            add.u32 %i, %i, 1;
            setp.lt.u32 %p0, %i, 4;
            @%p0 bra load_a_start;
        }

        // Offset our pointer by 64 floats = 256 bytes.
        add.u64 %pointerInA, %pointerInA, 256;
    }

    // Load matrix B into shared memory.
    {
        .reg .f32 %ftmp<8>;
        .reg .b64 %tmp<4>;

        mov.u64 %tmp0, %pointerInB;

        .reg .u32 %storePtr;
        mov.u32 %storePtr, %storePointerInSharedB;

        // Load all 16 rows, four rows at a time.
        .reg .u32 %i;
        mov.u32 %i, 0;
        {
        load_b_start:
            add.u64 %tmp1, %tmp0, %bytesPerRow;
            add.u64 %tmp2, %tmp1, %bytesPerRow;
            add.u64 %tmp3, %tmp2, %bytesPerRow;

            // Load four rows, two columns at a time.
            ld.global.f32 %ftmp0, [%tmp0];
            ld.global.f32 %ftmp1, [%tmp0+128];
            ld.global.f32 %ftmp2, [%tmp1];
            ld.global.f32 %ftmp3, [%tmp1+128];
            ld.global.f32 %ftmp4, [%tmp2];
            ld.global.f32 %ftmp5, [%tmp2+128];
            ld.global.f32 %ftmp6, [%tmp3];
            ld.global.f32 %ftmp7, [%tmp3+128];
    
            // Every other value, we offset (32+16)*4 rather than 32*4.
            st.shared.b32 [%storePtr], %ftmp0;
            st.shared.b32 [%storePtr+128], %ftmp1;
            st.shared.b32 [%storePtr+320], %ftmp2;
            st.shared.b32 [%storePtr+320+128], %ftmp3;
            st.shared.b32 [%storePtr+320*2], %ftmp4;
            st.shared.b32 [%storePtr+320*2+128], %ftmp5;
            st.shared.b32 [%storePtr+320*3], %ftmp6;
            st.shared.b32 [%storePtr+320*3+128], %ftmp7;

            add.u32 %storePtr, %storePtr, 1280; // add 320*4
            add.u64 %tmp0, %tmp3, %bytesPerRow;

            add.u32 %i, %i, 1;
            setp.lt.u32 %p0, %i, 4;
            @%p0 bra load_b_start;
        }

        add.u64 %pointerInB, %pointerInB, %loadBStride;
    }

    bar.sync 0;

    {
        .reg .b32 %a<32>;
        .reg .b32 %b<4>;

        // stride for A is 64+8, stride for B is 64+16.

        // Load the row of A exactly once.
        //    for i in range(8):
        //        j = i * 4
        //        offset = i * 32
        //        print(
        //            f"        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {{%a{j}, %a{j+1}, %a{j+2}, %a{j+3}}}, [%loadPointerInSharedA+{offset}], 72;"
        //        )
        //
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a0, %a1, %a2, %a3}, [%loadPointerInSharedA+0], 72;
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a4, %a5, %a6, %a7}, [%loadPointerInSharedA+32], 72;
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a8, %a9, %a10, %a11}, [%loadPointerInSharedA+64], 72;
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a12, %a13, %a14, %a15}, [%loadPointerInSharedA+96], 72;
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a16, %a17, %a18, %a19}, [%loadPointerInSharedA+128], 72;
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a20, %a21, %a22, %a23}, [%loadPointerInSharedA+160], 72;
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a24, %a25, %a26, %a27}, [%loadPointerInSharedA+192], 72;
        wmma.load.a.sync.aligned.row.m16n16k8.shared.tf32 {%a28, %a29, %a30, %a31}, [%loadPointerInSharedA+224], 72;

        // Each column of B gets a separate matmul.
        //    for col in range(4):
        //        for row in range(8):
        //            print(
        //                f"        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {{%b0, %b1, %b2, %b3}}, [%loadPointerInSharedB+{col*64}+2560*{row}], 80;"
        //            )
        //            out_start = col * 8
        //            a_start = row * 4
        //            out_str = ", ".join(f"%out{i}" for i in range(out_start, out_start + 8))
        //            print(
        //                f"        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {{{out_str}}}, {{%a{a_start}, %a{a_start+1}, %a{a_start+2}, %a{a_start+3}}}, {{%b0, %b1, %b2, %b3}}, {{{out_str}}};"
        //            )
        //
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*0], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*1], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a4, %a5, %a6, %a7}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*2], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a8, %a9, %a10, %a11}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*3], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a12, %a13, %a14, %a15}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*4], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a16, %a17, %a18, %a19}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*5], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a20, %a21, %a22, %a23}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*6], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a24, %a25, %a26, %a27}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+0+2560*7], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a28, %a29, %a30, %a31}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*0], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*1], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a4, %a5, %a6, %a7}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*2], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a8, %a9, %a10, %a11}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*3], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a12, %a13, %a14, %a15}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*4], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a16, %a17, %a18, %a19}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*5], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a20, %a21, %a22, %a23}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*6], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a24, %a25, %a26, %a27}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+64+2560*7], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, {%a28, %a29, %a30, %a31}, {%b0, %b1, %b2, %b3}, {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*0], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*1], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a4, %a5, %a6, %a7}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*2], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a8, %a9, %a10, %a11}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*3], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a12, %a13, %a14, %a15}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*4], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a16, %a17, %a18, %a19}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*5], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a20, %a21, %a22, %a23}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*6], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a24, %a25, %a26, %a27}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+128+2560*7], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, {%a28, %a29, %a30, %a31}, {%b0, %b1, %b2, %b3}, {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*0], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*1], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a4, %a5, %a6, %a7}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*2], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a8, %a9, %a10, %a11}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*3], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a12, %a13, %a14, %a15}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*4], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a16, %a17, %a18, %a19}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*5], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a20, %a21, %a22, %a23}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*6], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a24, %a25, %a26, %a27}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
        wmma.load.b.sync.aligned.row.m16n16k8.shared.tf32 {%b0, %b1, %b2, %b3}, [%loadPointerInSharedB+192+2560*7], 80;
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, {%a28, %a29, %a30, %a31}, {%b0, %b1, %b2, %b3}, {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31};
    }

    bar.sync 0;

    bra outer_loop;
outer_loop_end:

    {
        .reg .u64 %outColumn;
        .reg .u64 %outOffset;
        .reg .u64 %tmp;

        shl.b64 %outOffset, %stride, 8; // turn into a row offset (4 bytes), times 64 rows
        mul.lo.u64 %outOffset, %outOffset, %ctaY;

        // Offset for row
        cvt.u64.u32 %tmp, %tidY;
        mul.lo.u64 %tmp, %tmp, %stride;
        shl.b64 %tmp, %tmp, 6; // for second row: 16 * stride * 4 bytes
        add.u64 %outOffset, %outOffset, %tmp;

        shl.b64 %outColumn, %ctaX, 8; // 64 floats * 4 bytes
        add.u64 %outOffset, %outOffset, %outColumn;
        add.u64 %ptrOut, %ptrOut, %outOffset;

        // Copy to %ptrOut.
        .reg .u32 %stride32;
        cvt.u32.u64 %stride32, %stride;

        //
        //    for i in range(4):
        //        out = ", ".join(f"%out{i}" for i in range(i * 8, (i + 1) * 8))
        //        print(
        //            f"        wmma.store.d.sync.aligned.m16n16k16.global.row.f32 [%ptrOut+{i}*64], {{{out}}}, %stride32;"
        //        )
        //
        wmma.store.d.sync.aligned.m16n16k16.global.row.f32 [%ptrOut+0*64], {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, %stride32;
        wmma.store.d.sync.aligned.m16n16k16.global.row.f32 [%ptrOut+1*64], {%out8, %out9, %out10, %out11, %out12, %out13, %out14, %out15}, %stride32;
        wmma.store.d.sync.aligned.m16n16k16.global.row.f32 [%ptrOut+2*64], {%out16, %out17, %out18, %out19, %out20, %out21, %out22, %out23}, %stride32;
        wmma.store.d.sync.aligned.m16n16k16.global.row.f32 [%ptrOut+3*64], {%out24, %out25, %out26, %out27, %out28, %out29, %out30, %out31}, %stride32;
    }

    ret;
}