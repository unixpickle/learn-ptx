.version 7.0
.target sm_80 // needed for wmma instruction
.address_size 64

// Directly use wmma.load to load blocks from memory, save each block in contiguous
// shared memory, and then load blocks back when needed.

.visible .entry wmmaMatmulV7 (
    .param .u64 ptrA,
    .param .u64 ptrB,
    .param .u64 ptrOut,
    .param .u32 numBlocks
) {
    .reg .pred %p0;

    // Attributes of the thread/CTA.
    .reg .u32 %tidX;
    .reg .u32 %tidY;
    .reg .u64 %ctaX;
    .reg .u64 %ctaY;

    // Arguments
    .reg .u64 %ptrA;
    .reg .u64 %ptrB;
    .reg .u64 %ptrOut;
    .reg .u32 %numBlocks;

    // Cache for block operands.
    .shared .align 4 .f32 sharedA[4096];
    .shared .align 4 .f32 sharedB[4096];

    ld.param.u64 %ptrA, [ptrA];
    ld.param.u64 %ptrB, [ptrB];
    ld.param.u64 %ptrOut, [ptrOut];
    ld.param.u32 %numBlocks, [numBlocks];

    mov.u32 %tidX, %tid.x; // index in warp (0-32)
    mov.u32 %tidY, %tid.y; // warp index in block (0-16)
    cvt.u64.u32 %ctaX, %ctaid.x; // column of output
    cvt.u64.u32 %ctaY, %ctaid.y; // row of output

    // Accumulation registers are stored as 8 floats per thread.
    .reg .f32 %out<8>;
    mov.f32 %out0, 0.0;
    mov.f32 %out1, 0.0;
    mov.f32 %out2, 0.0;
    mov.f32 %out3, 0.0;
    mov.f32 %out4, 0.0;
    mov.f32 %out5, 0.0;
    mov.f32 %out6, 0.0;
    mov.f32 %out7, 0.0;

    // The row-wise stride of the matrices, measured in tf32's.
    .reg .u64 %stride;
    {
        .reg .u64 %tmp;
        cvt.u64.u32 %tmp, %numBlocks;
        shl.b64 %stride, %tmp, 6;
    }
    .reg .u32 %stride32;
    cvt.u32.u64 %stride32, %stride;

    // Stride of 64 rows.
    .reg .u64 %loadBStride;
    shl.b64 %loadBStride, %stride, 8; // 64 rows * (4 bytes/float)

    // We will use pointerInA to point to a warp-specific part of ptrA.
    // Our block in A is made up of 4 rows and 8 columns.
    // Each warp will load two columns from a given row.
    // So warp 0 will load the first 16x16 block, then warp 1 will load
    // the next 16x16 block, in row-major order.
    // Each block is stored as 32*4 floats (first fragment first, then second
    // fragment, etc.). And then the second block (next 8 columns) follows.
    // We set storePointerInSharedA to the thread-specific location where the
    // first loaded fragment is to be stored.
    // We set loadPointerInSharedA to the first entry in this warp's specific
    // row of A.
    .reg .u64 %pointerInA;
    .reg .u32 %storePointerInSharedA;
    .reg .u32 %loadPointerInSharedA;
    {
        .reg .u64 %tmp<2>;
        .reg .u32 %stmp0;

        cvt.u64.u32 %tmp0, %tidY;
        shr.b64 %tmp1, %tmp0, 2; // tidY // 4
        and.b64 %tmp0, %tmp0, 3; // tidY % 4
        shl.b64 %pointerInA, %ctaY, 2; // 4 16x16 blocks per outer row
        add.u64 %tmp1, %tmp1, %pointerInA;
        mul.lo.u64 %pointerInA, %tmp1, %stride;
        add.u64 %pointerInA, %tmp0, %pointerInA;
        shl.b64 %pointerInA, %pointerInA, 6; // *= (16 floats) * (4 bytes/float)
        add.u64 %pointerInA, %pointerInA, %ptrA; // &ptrA[16*((tidY % 4) + stride*(tidY // 4 + 4*ctaY))]

        // We load from a row of A, given by tidY // 4.
        shr.b32 %stmp0, %tidY, 2; // tidY // 4
        shl.b32 %stmp0, %stmp0, 12; // *= (16*16 floats) * (4 bytes/float) * (4 columns)
        mov.u32 %loadPointerInSharedA, sharedA;
        add.u32 %loadPointerInSharedA, %loadPointerInSharedA, %stmp0;

        // We will store a 16x16 block somewhere in shared memory.
        // The offset will be tidY*(16*16 floats * 4 bytes/float)
        shl.b32 %stmp0, %tidY, 10; // 16*16*4
        mov.u32 %storePointerInSharedA, sharedA;
        add.u32 %storePointerInSharedA, %storePointerInSharedA, %stmp0;

        // Offset in shared memory by tidX * (4 bytes/float).
        shl.b32 %stmp0, %tidX, 2;
        add.u32 %loadPointerInSharedA, %loadPointerInSharedA, %stmp0;
        add.u32 %storePointerInSharedA, %storePointerInSharedA, %stmp0;
    }

    // We can load one 8x16 chunk of B at a time, so we need to load
    // four columns and 8 rows.
    // We will make each warp responsible for loading a 8x32 block.
    // We store this memory in a block-transpose order, so the first
    // 16x8 block is pointed to by storePointerInSharedB, and the second
    // is offset by 64*16 floats.
    // loadPointerInSharedB will point to the first block in the column
    // that this thread is responsible for loading.
    .reg .u64 %pointerInB;
    .reg .u32 %storePointerInSharedB;
    .reg .u32 %loadPointerInSharedB;
    {
        .reg .u32 %stmp<2>;
        .reg .u64 %tmp<2>;

        shl.b64 %tmp0, %ctaX, 8; // 4 bytes per float * 64 columns
        add.u64 %pointerInB, %ptrB, %tmp0;
        // Warp column offset is given by (tidY % 2) * (32 floats) * (4 bytes/float)
        cvt.u64.u32 %tmp1, %tidY;
        and.b64 %tmp0, %tmp1, 1;
        shl.b64 %tmp0, %tmp0, 7; // *= 128
        add.u64 %pointerInB, %pointerInB, %tmp0; // pointerInB += (tidY % 2) * (32 floats) * (4 bytes/float)
        // Warp row offset is given by (tidY // 2) * (8 rows) * (stride floats/row) * (4 bytes/float)
        shr.b64 %tmp0, %tmp1, 1;
        mul.lo.u64 %tmp0, %tmp0, %stride; // *= stride floats/row
        shl.b64 %tmp0, %tmp0, 5; // *= (8 rows) * (4 bytes/float)
        add.u64 %pointerInB, %pointerInB, %tmp0; // pointerInB += (tidY // 2) stride * (8 rows) * (4 bytes/float)

        // Our column in the destination is given by tidY % 2
        // Our row is given by tidY // 2
        // The offset of the warp, since we use column-major order, is
        //     (column*16 + row) * (16*8 floats * (4 bytes/float))
        // Note that the warp writes two columns, this is just the first.
        // The second column will be offset by (64 * 16 floats * (4 bytes/float)) = 4096 bytes.
        // Hence why column is multiplied by 16 instead of 8, when there are only 8 rows.
        and.b32 %stmp0, %tidY, 1; // column
        shr.b32 %stmp1, %tidY, 1; // row
        shl.b32 %stmp0, %stmp0, 4; // column * 16
        add.u32 %stmp0, %stmp0, %stmp1; // stmp0 = column*8 + row
        shl.b32 %stmp0, %stmp0, 9; // *= (16*8 floats) * (4 bytes/float)
        mov.u32 %storePointerInSharedB, sharedB;
        add.u32 %storePointerInSharedB, %storePointerInSharedB, %stmp0;

        // When we load a value, we only look at the column we are working
        // on, which is given by tidY % 4.
        // Each column contains (64 * 16 floats) * (4 bytes/float)
        and.b32 %stmp0, %tidY, 3;
        shl.b32 %stmp0, %stmp0, 12; // (64 * 16 floats) * (4 bytes/float) = 4096
        mov.u32 %loadPointerInSharedB, sharedB;
        add.u32 %loadPointerInSharedB, %loadPointerInSharedB, %stmp0;

        // Offset in shared memory by tidX * (4 bytes/float).
        shl.b32 %stmp0, %tidX, 2;
        add.u32 %loadPointerInSharedB, %loadPointerInSharedB, %stmp0;
        add.u32 %storePointerInSharedB, %storePointerInSharedB, %stmp0;
    }

    .reg .u32 %remainingIters;
    mov.u32 %remainingIters, %numBlocks;

outer_loop:
    setp.gt.u32 %p0, %remainingIters, 0;
    @!%p0 bra outer_loop_end;
    sub.u32 %remainingIters, %remainingIters, 1;

    // Load matrix A into shared memory.

    {
        .reg .b32 %ftmp<4>;

        // Load first 16x8 chunk.
        wmma.load.a.sync.aligned.row.m16n16k8.global.tf32 {%ftmp0, %ftmp1, %ftmp2, %ftmp3}, [%pointerInA], %stride32;
        st.shared.b32 [%storePointerInSharedA], %ftmp0;
        st.shared.b32 [%storePointerInSharedA+128], %ftmp1;
        st.shared.b32 [%storePointerInSharedA+256], %ftmp2;
        st.shared.b32 [%storePointerInSharedA+384], %ftmp3;

        // Load second 16x8 chunk.
        // Results are stored offset by 16*8*4 bytes.
        wmma.load.a.sync.aligned.row.m16n16k8.global.tf32 {%ftmp0, %ftmp1, %ftmp2, %ftmp3}, [%pointerInA+32], %stride32;
        st.shared.b32 [%storePointerInSharedA+512], %ftmp0;
        st.shared.b32 [%storePointerInSharedA+640], %ftmp1;
        st.shared.b32 [%storePointerInSharedA+768], %ftmp2;
        st.shared.b32 [%storePointerInSharedA+896], %ftmp3;

        // Offset our pointer by 64 floats = 256 bytes.
        add.u64 %pointerInA, %pointerInA, 256;
    }

    // Load matrix B into shared memory.
    {
        .reg .b32 %ftmp<4>;

        // Load first 8x16 chunk.
        wmma.load.b.sync.aligned.row.m16n16k8.global.tf32 {%ftmp0, %ftmp1, %ftmp2, %ftmp3}, [%pointerInB], %stride32;
        st.shared.f32 [%storePointerInSharedB], %ftmp0;
        st.shared.f32 [%storePointerInSharedB+128], %ftmp1;
        st.shared.f32 [%storePointerInSharedB+256], %ftmp2;
        st.shared.f32 [%storePointerInSharedB+384], %ftmp3;

        // Load second 8x16 chunk.
        // This is offset by a 64x16 column of floats, so 64*16*4=4096 bytes.
        // Results are stored offset by 16*8*4 bytes.
        wmma.load.b.sync.aligned.row.m16n16k8.global.tf32 {%ftmp0, %ftmp1, %ftmp2, %ftmp3}, [%pointerInB+64], %stride32;
        st.shared.f32 [%storePointerInSharedB+4096], %ftmp0;
        st.shared.f32 [%storePointerInSharedB+4224], %ftmp1;
        st.shared.f32 [%storePointerInSharedB+4352], %ftmp2;
        st.shared.f32 [%storePointerInSharedB+4480], %ftmp3;

        // Offset our pointer by 64 rows.
        add.u64 %pointerInB, %pointerInB, %loadBStride;
    }

    bar.sync 0;

    {
        .reg .b32 %a<4>;
        .reg .b32 %b<4>;

        // Generated by this Python code:
        //
        // for offset in range(8):
        //    print(f"// Offset by {offset} block(s) of size 8x16 or 16x8")
        //    offset0 = offset * 512
        //    offset1 = offset * 512 + 128
        //    offset2 = offset * 512 + 128 * 2
        //    offset3 = offset * 512 + 128 * 3
        //    for i in range(4):
        //        print(f"ld.shared.f32 %a{i}, [%loadPointerInSharedA+{offset*512+128*i}]")
        //        print(f"ld.shared.f32 %b{i}, [%loadPointerInSharedB+{offset*512+128*i}]")
        //    print(
        //        "wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};"
        //    )
        //    print("")

        // Offset by 0 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+0];
        ld.shared.b32 %b0, [%loadPointerInSharedB+0];
        ld.shared.b32 %a1, [%loadPointerInSharedA+128];
        ld.shared.b32 %b1, [%loadPointerInSharedB+128];
        ld.shared.b32 %a2, [%loadPointerInSharedA+256];
        ld.shared.b32 %b2, [%loadPointerInSharedB+256];
        ld.shared.b32 %a3, [%loadPointerInSharedA+384];
        ld.shared.b32 %b3, [%loadPointerInSharedB+384];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};

        // Offset by 1 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+512];
        ld.shared.b32 %b0, [%loadPointerInSharedB+512];
        ld.shared.b32 %a1, [%loadPointerInSharedA+640];
        ld.shared.b32 %b1, [%loadPointerInSharedB+640];
        ld.shared.b32 %a2, [%loadPointerInSharedA+768];
        ld.shared.b32 %b2, [%loadPointerInSharedB+768];
        ld.shared.b32 %a3, [%loadPointerInSharedA+896];
        ld.shared.b32 %b3, [%loadPointerInSharedB+896];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};

        // Offset by 2 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+1024];
        ld.shared.b32 %b0, [%loadPointerInSharedB+1024];
        ld.shared.b32 %a1, [%loadPointerInSharedA+1152];
        ld.shared.b32 %b1, [%loadPointerInSharedB+1152];
        ld.shared.b32 %a2, [%loadPointerInSharedA+1280];
        ld.shared.b32 %b2, [%loadPointerInSharedB+1280];
        ld.shared.b32 %a3, [%loadPointerInSharedA+1408];
        ld.shared.b32 %b3, [%loadPointerInSharedB+1408];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};

        // Offset by 3 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+1536];
        ld.shared.b32 %b0, [%loadPointerInSharedB+1536];
        ld.shared.b32 %a1, [%loadPointerInSharedA+1664];
        ld.shared.b32 %b1, [%loadPointerInSharedB+1664];
        ld.shared.b32 %a2, [%loadPointerInSharedA+1792];
        ld.shared.b32 %b2, [%loadPointerInSharedB+1792];
        ld.shared.b32 %a3, [%loadPointerInSharedA+1920];
        ld.shared.b32 %b3, [%loadPointerInSharedB+1920];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};

        // Offset by 4 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+2048];
        ld.shared.b32 %b0, [%loadPointerInSharedB+2048];
        ld.shared.b32 %a1, [%loadPointerInSharedA+2176];
        ld.shared.b32 %b1, [%loadPointerInSharedB+2176];
        ld.shared.b32 %a2, [%loadPointerInSharedA+2304];
        ld.shared.b32 %b2, [%loadPointerInSharedB+2304];
        ld.shared.b32 %a3, [%loadPointerInSharedA+2432];
        ld.shared.b32 %b3, [%loadPointerInSharedB+2432];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};

        // Offset by 5 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+2560];
        ld.shared.b32 %b0, [%loadPointerInSharedB+2560];
        ld.shared.b32 %a1, [%loadPointerInSharedA+2688];
        ld.shared.b32 %b1, [%loadPointerInSharedB+2688];
        ld.shared.b32 %a2, [%loadPointerInSharedA+2816];
        ld.shared.b32 %b2, [%loadPointerInSharedB+2816];
        ld.shared.b32 %a3, [%loadPointerInSharedA+2944];
        ld.shared.b32 %b3, [%loadPointerInSharedB+2944];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};

        // Offset by 6 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+3072];
        ld.shared.b32 %b0, [%loadPointerInSharedB+3072];
        ld.shared.b32 %a1, [%loadPointerInSharedA+3200];
        ld.shared.b32 %b1, [%loadPointerInSharedB+3200];
        ld.shared.b32 %a2, [%loadPointerInSharedA+3328];
        ld.shared.b32 %b2, [%loadPointerInSharedB+3328];
        ld.shared.b32 %a3, [%loadPointerInSharedA+3456];
        ld.shared.b32 %b3, [%loadPointerInSharedB+3456];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};

        // Offset by 7 block(s) of size 8x16 or 16x8
        ld.shared.b32 %a0, [%loadPointerInSharedA+3584];
        ld.shared.b32 %b0, [%loadPointerInSharedB+3584];
        ld.shared.b32 %a1, [%loadPointerInSharedA+3712];
        ld.shared.b32 %b1, [%loadPointerInSharedB+3712];
        ld.shared.b32 %a2, [%loadPointerInSharedA+3840];
        ld.shared.b32 %b2, [%loadPointerInSharedB+3840];
        ld.shared.b32 %a3, [%loadPointerInSharedA+3968];
        ld.shared.b32 %b3, [%loadPointerInSharedB+3968];
        wmma.mma.sync.aligned.row.row.m16n16k8.f32.tf32.tf32.f32 {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, {%a0, %a1, %a2, %a3}, {%b0, %b1, %b2, %b3}, {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7};
    }

    bar.sync 0;

    bra outer_loop;
outer_loop_end:

    {
        .reg .u64 %outColumn;
        .reg .u64 %outOffset;
        .reg .u64 %tmp;

        shl.b64 %outColumn, %ctaX, 8; // 64 floats * 4 bytes
        cvt.u64.u32 %tmp, %tidY;
        and.b64 %tmp, %tmp, 3; // Offset for output column
        shl.b64 %tmp, %tmp, 6; // 16 floats * 4 bytes
        add.u64 %outColumn, %outColumn, %tmp;

        shl.b64 %outOffset, %stride, 8; // turn into a row offset (4 bytes), times 64 rows
        mul.lo.u64 %outOffset, %outOffset, %ctaY;

        // Offset for row
        cvt.u64.u32 %tmp, %tidY;
        and.b64 %tmp, %tmp, 12; // Row offset times 4
        mul.lo.u64 %tmp, %tmp, %stride;
        shl.b64 %tmp, %tmp, 4; // for second row: 16 * stride * 4 bytes / (4 for the offset of tidY row)
        add.u64 %outOffset, %outOffset, %tmp;

        add.u64 %outOffset, %outOffset, %outColumn;
        add.u64 %ptrOut, %ptrOut, %outOffset;

        // Copy to %ptrOut.
        .reg .u32 %stride32;
        cvt.u32.u64 %stride32, %stride;
        wmma.store.d.sync.aligned.m16n16k16.global.row.f32 [%ptrOut], {%out0, %out1, %out2, %out3, %out4, %out5, %out6, %out7}, %stride32;
    }

    ret;
}